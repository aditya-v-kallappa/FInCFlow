# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gib4L1u2UNMGZwOZqvtj8OGWPySv4OFD
"""

import torch
import torch.nn as nn
import torchvision
import torch.nn.functional as F
import torch.optim as optim
import torch.distributions as D
import math
import torchvision.transforms as transforms
import time
from model import Model2

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5), (0.5))
    ])

batch_size = 10000

trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.MNIST(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

# classes = ('plane', 'car', 'bird', 'cat',
#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

device = "cpu" if not torch.cuda.is_available() else "cuda"

const_k = 0.5 * math.log(2.0 * math.pi) 

def _log_prob(loc, scale, value):
  # # if self._validate_args:
  # #     self._validate_sample(value)
  # # compute the variance
  # # var = (scale ** 2)
  # var = scale * scale
  # log_scale = math.log(scale) #if isinstance(scale, ) else scale.log()
  # return -((value - loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))

  """
  value : B X d
  """
  loss = []
  for v in value:
    loss.append(const_k + 0.5 * (torch.linalg.norm(v) ** 2))
  loss = torch.tensor(loss, requires_grad=True).reshape(value.shape[0], -1).to(device)
  # print("Loss Shpae:", loss.shape)
  return loss







class Model(nn.Module):
    def __init__(self, n_input_channels):
      super().__init__()

      self.pad1 = nn.ConstantPad2d((2, 0, 2, 0), 0)
      self.conv1 = nn.Conv2d(n_input_channels, 1, (3, 3), 1, padding=0, bias=False)
      self.init_weight()
      # self.relu1 = nn.ReLU
      self.flatten = torch.flatten

      self.register_buffer('base_dist_mean', torch.zeros(1))
      self.register_buffer('base_dist_var', torch.ones(1))

      # self.linear1 = nn.Linear(100, 10)
      # self.linear2 = nn.Linear()
    def init_weight(self):
      _shape = self.conv1.weight.data.shape 
      # temp = (torch.arange(1, 10).reshape(self.conv1.weight.shape).to(dtype=self.conv1.weight.dtype)) * .1
      # print("Shape of weight:", temp.shape)
      self.conv1.weight.data[:, -1, -1, -1] = torch.tensor([1.0]).to(dtype=self.conv1.weight.dtype)

    def forward(self, x, bits_per_pixel=False):
      # B, C, D, H, W = x.shape
      non_padded_x = x
      # print("Before Forward Pass")
      # print("Conv weight:\n", self.conv1.weight)
      x = self.pad1(x)
      # print("Padded Y:\n", x)
      x = self.conv1(x)
      # x = self.relu1(x)
      x = self.flatten(x, 1, -1)
      # element = self.conv1.weight.flatten()[-1]
      # logdet = torch.log(torch.abs(element)) * non_padded_x[0].numel() # C * D* H * W
    #   logdet = 0
      # print("Size of output: ", x.shape)
      log_prob = _log_prob(loc=0, scale=1, value=x)
      # if bits_per_pixel:
      #   log_prob /= (math.log(2) * x[0].numel())
      # print("Log_prob Shape:", log_prob.shape)
      return x, -log_prob

    def clear_grad(self):
        self.conv1.weight.grad[:, -1, -1, -1] = 0.0

    @property
    def base_dist(self):
        return D.Normal(self.base_dist_mean, self.base_dist_var)

    def log_prob(self, x, bits_per_pixel=False):
        zs, logdet = self.forward(x)
        # print(zs)
        # print(logdet)
        # log_prob = sum(D.Normal(0, 1).log_prob(z).sum() for z in zs) + logdet 
        # log_prob = sum(self.base_dist.log_prob(z).sum() for z in zs) + logdet 
        log_prob = sum(_log_prob(self.base_dist_mean, self.base_dist_var, z).sum() for z in zs) + logdet
        # log_prob = zs.sum() + logdet 
        if bits_per_pixel:
            log_prob /= (math.log(2) * x[0].numel())
        return log_prob

    def gaussianize(self, x):
      pass


# device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "cpu"
# if torch.cuda.device_count() > 1:
#     net = nn.DataParallel(Model(1))
# else:
# net = Model(1).to(device)
net = Model2(20).to(device)
# print("Model: \n", net)
# criterion = nn.CrossEntropyLoss()
criterion = nn.MSELoss(reduction='mean')
# criterion = nn.L1Loss(reduction='mean')
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
print("Training Started on ", device)
for epoch in range(10000):  # loop over the dataset multiple times
    net.train()
    running_loss = 0.0
    _time = time.time()
    # r_data = torch.rand(size=(200, 1, 10, 10))

    for i, data in enumerate(trainloader, 0):
        
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        
        # forward + backward + optimize
        # outputs, log_prob = net(inputs)
        # loss = criterion(outputs)
        outputs, _ = net(inputs)
        # print("shape:", outputs.shape)
        target = torch.zeros_like(outputs).to(device)
        # loss = const_k + 0.5 * criterion(outputs, target) * 3.321928095 / (28 * 28)
        loss = criterion(outputs, target)
        
        # loss = torch.tensor(1.0, requires_grad=True)
        # (-1 * net.log_prob(inputs, True).mean(0)).backward(retain_graph=True)
        # loss = -net.log_prob(inputs, True)
        loss.backward()
        # print("Conv grade:", net.conv1.weight.grad)
        # net.conv1.weight.grad[:, -1, -1, -1] = 0.0
        net.clear_grad()
        # print("Grad:\n", net.conv1.weight.grad)
        # print("Weight:\n", net.conv1.weight)
        # print("shape: ", net.conv1.weight.grad.shape)
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        # if i % 200 == 199:    # print every 2000 mini-batches
        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.10f}, time: {time.time() - _time}')
        running_loss = 0.0
        _time = time.time()

torch.save(net.state_dict(), './saved_model_normalized.pt')
    # print(running_loss)
print('Finished Training')

# temp = iter(trainloader)
# print(temp.next())



# def construct_matrix(weight, input_shape):
#     C, H, W = input_shape
#     w = weight.squeeze()
#     W = torch.zeros(size=(C * H * W, C * H * W))
#     W[0:4, 0:4] = torch.tensor([[w[2, 2], 0, 0, 0],
#                                [w[2, 1], w[2, 2], 0, 0],
#                                [w[2, 0], w[2, 1], w[2, 2], 0],
#                                [0, w[2, 0], w[2, 1], w[2, 2]]])
#     W[4:8, 4:8], W[8:12, 8:12], W[12:16, 12:16] = W[0:4, 0:4], W[0:4, 0:4], W[0:4, 0:4]

#     W[4:8, 0:4] = torch.tensor([[w[1, 2], 0, 0, 0],
#                                [w[1, 1], w[1, 2], 0, 0],
#                                [w[1, 0], w[1, 1], w[1, 2], 0],
#                                [0, w[1, 0], w[1, 1], w[1, 2]]])
#     W[8:12, 4:8], W[12:16, 8:12] = W[4:8, 0:4], W[4:8, 0:4]


#     W[8:12, 0:4] = torch.tensor([[w[0, 2], 0, 0, 0],
#                                [w[0, 1], w[0, 2], 0, 0],
#                                [w[0, 0], w[0, 1], w[0, 2], 0],
#                                [0, w[0, 0], w[0, 1], w[0, 2]]])
#     W[12:16, 4:8] = W[8:12, 0:4]

#     return W

# dummy_input = torch.arange(1, 17).reshape(1, 1, 4, 4).to(torch.float32).to(device) / 255

# print(dummy_input)

# model = Model().to(device)

# y1, _ = model(dummy_input)
# y1 = y1.to(device)
# print("Output of Conv:\n", y1)

# W = construct_matrix(model.conv1.weight, (1, 4, 4))

# W = W.to(model.conv1.weight.dtype).to(device)
# # print(W)

# x = torch.linalg.solve(W, y1)
# print("Inverse Convolution Result:\n", x)

# logdet = x.numel() * torch.log(torch.abs(W[0, 0]))
# print(logdet)
# base_dist = torch.distributions.normal.Normal(0, 1)

# sample = base_dist.sample(sample_shape=y1.shape).to(device)

# calculated_x = torch.linalg.solve(W, sample).reshape(dummy_input.shape)
# calculated_sample, _ = model(calculated_x)

# print(sample)
# print(calculated_sample)

# # print(torch.sqrt((sample.flatten() - torch.tensor(calculated_sample))**2).mean())

